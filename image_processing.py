# -*- coding: utf-8 -*-
"""image_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ggkQrKAyEl6k3bjEqrPH83bSd3KLQXQa
"""

from google.colab import drive
drive.mount('/content/drive/')

import os
import numpy as np
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import cv2

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

def extract_features(img_path):
    """Extracts features from an image using the VGG16 model."""
    img = load_img(img_path, target_size=(224, 224))
    img_array = img_to_array(img)
    expanded_img_array = np.expand_dims(img_array, axis=0)
    preprocessed_img = preprocess_input(expanded_img_array)
    features = base_model.predict(preprocessed_img)
    flattened_features = features.flatten()
    return flattened_features

base_folder_path = "/content/drive/My Drive/images"
sub_folders = ['covid', 'pnemonia', 'tuberclousis']
image_paths = []

for sub_folder in sub_folders:
    folder_path = os.path.join(base_folder_path, sub_folder)
    image_paths.extend([os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(('.jpg', '.jpeg', '.png')) and not filename.startswith('._')])

all_features = []
all_image_paths = []

for img_path in image_paths:
    img = cv2.imread(img_path)
    if img is None:  # If the image cannot be read or opened
        print(f"Skipping invalid image file {img_path}")
        continue
    try:
        feat = extract_features(img_path)
        all_features.append(feat)
        all_image_paths.append(img_path)
    except Exception as e:
        print(f"Error processing {img_path}: {e}")

if len(all_features) == 0:
    print("No features extracted. Check the file paths and extraction process.")
    exit()

all_features = np.array(all_features)

k = 3
kmeans = KMeans(n_clusters=k, random_state=0).fit(all_features)

fig, axs = plt.subplots(k, figsize=(5, 5*k))
for i in range(k):
    clustered_image_paths = [all_image_paths[j] for j, cluster_num in enumerate(kmeans.labels_) if cluster_num == i]
    img = cv2.imread(clustered_image_paths[0])
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    axs[i].imshow(img_rgb)
    axs[i].axis('off')

plt.show()

for i in range(k):
    clustered_image_paths = [all_image_paths[j] for j, cluster_num in enumerate(kmeans.labels_) if cluster_num == i]
    # Display first 5 images from each cluster to understand commonality
    for img_path in clustered_image_paths[:5]:
        img = cv2.imread(img_path)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.imshow(img_rgb)
        plt.axis('off')
        plt.show()

labels_dict = {0: "Disease A", 1: "Disease B", 2: "Disease C"}

labeled_data = []
for i, path in enumerate(all_image_paths):
    cluster_label = kmeans.labels_[i]
    disease_label = labels_dict[cluster_label]
    labeled_data.append((path, disease_label))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from sklearn.model_selection import train_test_split

# Splitting data
paths, labels = zip(*labeled_data)
train_paths, test_paths, train_labels, test_labels = train_test_split(paths, labels, test_size=0.2)

# Converting labels to one-hot encoding
label_to_index = {v: i for i, v in enumerate(labels_dict.values())}
train_labels_encoded = tf.keras.utils.to_categorical([label_to_index[label] for label in train_labels])
test_labels_encoded = tf.keras.utils.to_categorical([label_to_index[label] for label in test_labels])

# Loading and preprocessing images
def load_and_preprocess_image(path):
    img = cv2.imread(path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_resized = cv2.resize(img_rgb, (224, 224))
    return img_resized / 255.0

train_images = np.array([load_and_preprocess_image(path) for path in train_paths])
test_images = np.array([load_and_preprocess_image(path) for path in test_paths])

# CNN Model
cnn_model = Sequential()
cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
cnn_model.add(MaxPooling2D(2, 2))
cnn_model.add(Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D(2, 2))
cnn_model.add(Conv2D(128, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D(2, 2))
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dense(len(labels_dict), activation='softmax'))

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Training the model
cnn_model.fit(train_images, train_labels_encoded, epochs=5, validation_data=(test_images, test_labels_encoded))

new_image_path = "/content/1B734A89-A1BF-49A8-A1D3-66FAFA4FAC5D.jpeg"
new_image = load_and_preprocess_image(new_image_path)
new_image_expanded = np.expand_dims(new_image, axis=0)

predictions = cnn_model.predict(new_image_expanded)
predicted_label = labels_dict[np.argmax(predictions)]
print(f"Predicted Disease: {predicted_label}")

import numpy as np
import matplotlib.pyplot as plt

# Predictions
predictions = cnn_model.predict(test_images)
predicted_labels = np.argmax(predictions, axis=1)

# Convert one-hot encoded labels back to label names
true_labels = np.argmax(test_labels_encoded, axis=1)

# Plot some of the test images along with their predicted and true labels
num_images_to_show = 10

for i in range(num_images_to_show):
    plt.figure(figsize=(5,2))
    plt.imshow(test_images[i])
    plt.title(f"Predicted: {labels_dict[predicted_labels[i]]}, True: {labels_dict[true_labels[i]]}")
    plt.axis('off')
    plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Calculate confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=labels_dict.values(), yticklabels=labels_dict.values())
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

import matplotlib.pyplot as plt

# Train the model and store the training histo
history = cnn_model.fit(train_images, train_labels_encoded, epochs=5, validation_data=(test_images, test_labels_encoded))




# Plot training & validation loss values
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()# Plot training & validation accuracy values
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

mlp_model = Sequential([
    Dense(512, activation='relu', input_shape=(all_features.shape[1],)),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(k, activation='softmax')  # 'k' is the number of clusters
])

mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Convert cluster labels to one-hot encoding for training the model
one_hot_labels = tf.keras.utils.to_categorical(kmeans.labels_, num_classes=k)

# Train the model
history = mlp_model.fit(all_features, one_hot_labels, epochs=5, batch_size=32, validation_split=0.2)

import matplotlib.pyplot as plt

# Extract accuracy and loss from the history object
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 5))

# Plot training and validation accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.show()

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Split the data into training and validation sets (assuming you haven't done this manually)
train_features, val_features, train_labels, val_labels = train_test_split(all_features, one_hot_labels, test_size=0.2, random_state=42)

# 2. Obtain predictions on the validation set
mlp_predictions = mlp_model.predict(val_features)
predicted_classes_mlp = np.argmax(mlp_predictions, axis=1)
true_classes = np.argmax(val_labels, axis=1)

# 3. Generate the confusion matrix
conf_matrix_mlp = confusion_matrix(true_classes, predicted_classes_mlp)

# 4. Visualize the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_mlp, annot=True, fmt='g', cmap='Blues')
plt.xlabel('Predicted Classes')
plt.ylabel('True Classes')
plt.title('Confusion Matrix for MLP Model')
plt.show()

# @title
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D

# Load base model: ResNet50
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# To make sure the pre-trained weights remain intact, we freeze the layers of the base model.
for layer in base_model.layers:
    layer.trainable = False

# Adding our custom layers at the end for classification
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
predictions = Dense(len(labels_dict), activation='softmax')(x)

# Final model
resnet_model = Model(inputs=base_model.input, outputs=predictions)

resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train
resnet_model.fit(train_images, train_labels_encoded, epochs=5, validation_data=(test_images, test_labels_encoded))
# Plotting Training and Validation Accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plotting Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()



import matplotlib.pyplot as plt
history = resnet_model.fit(train_images, train_labels_encoded, epochs=5, validation_data=(test_images, test_labels_encoded))


# Plotting Training and Validation Accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

# Plotting Training and Validation Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you've already loaded and preprocessed your data, and trained your resnet50_model

# 1. Predict classes for the test set using the ResNet50 model.
resnet50_predictions = resnet50_model.predict(test_images)
predicted_classes_resnet = np.argmax(resnet50_predictions, axis=1)
true_classes = np.argmax(test_labels_encoded, axis=1)

# 2. Generate the confusion matrix.
conf_matrix_resnet = confusion_matrix(true_classes, predicted_classes_resnet)

# 3. Visualize the confusion matrix.
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_resnet, annot=True, fmt='g', cmap='Blues',
            xticklabels=labels_dict.values(),
            yticklabels=labels_dict.values())
plt.xlabel('Predicted Classes')
plt.ylabel('True Classes')
plt.title('Confusion Matrix for ResNet50 Model')
plt.show()

import matplotlib.pyplot as plt

# Plotting Training Accuracy
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(cnn_model.history.history['accuracy'], label='CNN Training Accuracy', color='blue')
plt.plot(resnet_model.history.history['accuracy'], label='ResNet Training Accuracy', color='green')
plt.plot(mlp_model.history.history['accuracy'], label='MLP Training Accuracy', color='red')
plt.title('Training Accuracy')
plt.legend()

# Plotting Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(cnn_model.history.history['val_accuracy'], label='CNN Validation Accuracy', color='blue')
plt.plot(resnet_model.history.history['val_accuracy'], label='ResNet Validation Accuracy', color='green')
plt.plot(mlp_model.history.history['val_accuracy'], label='MLP Validation Accuracy', color='red')
plt.title('Validation Accuracy')
plt.legend()
plt.show()

# Plotting Training Loss
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(cnn_model.history.history['loss'], label='CNN Training Loss', color='blue')
plt.plot(resnet_model.history.history['loss'], label='ResNet Training Loss', color='green')
plt.plot(mlp_model.history.history['loss'], label='MLP Training Loss', color='red')
plt.title('Training Loss')
plt.legend()

# Plotting Validation Loss
plt.subplot(1, 2, 2)
plt.plot(cnn_model.history.history['val_loss'], label='CNN Validation Loss', color='blue')
plt.plot(resnet_model.history.history['val_loss'], label='ResNet Validation Loss', color='green')
plt.plot(mlp_model.history.history['val_loss'], label='MLP Validation Loss', color='red')
plt.title('Validation Loss')
plt.legend()
plt.show()